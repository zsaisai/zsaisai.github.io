<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://gitee.com/z_saisai/ware_01/raw/master/blog_account/logo2.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://gitee.com/z_saisai/ware_01/raw/master/blog_account/logo2.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zsaisai.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="1. 概念区分Loss Function损失函数 Loss Function  损失函数 Loss Function还分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，通常是针对单个训练样本而言，给定一个模型输出  y^ 和一个真实 y ，损失函数输出一个实值损失y - y^ 。用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型">
<meta property="og:type" content="article">
<meta property="og:title" content="损失函数">
<meta property="og:url" content="https://zsaisai.github.io/Loss%20Function/index.html">
<meta property="og:site_name" content="ZhangSaiSai">
<meta property="og:description" content="1. 概念区分Loss Function损失函数 Loss Function  损失函数 Loss Function还分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，通常是针对单个训练样本而言，给定一个模型输出  y^ 和一个真实 y ，损失函数输出一个实值损失y - y^ 。用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=J_%7BMSE%7D+=+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D(y_i+-+%5Chat%7By_i%7D)%5E2+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmu=0,+%5Csigma=1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+(-%5Cfrac%7B(y_i-%5Chat%7By_i%7D)%5E2%7D%7B2%7D%5Cright+)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(y_i+%5Cvert+x_i)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x,+y)+=+%5Cprod_%7Bi=1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+(-%5Cfrac%7B(y_i-%5Chat%7By_i%7D)%5E2%7D%7B2%7D%5Cright)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=LL(x,+y)=%5Cmathbb%7Blog%7D(L(x,+y))=-%5Cfrac%7BN%7D%7B2%7D%5Cmathbb%7Blog%7D2%5Cpi+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi=1%7D%5E%7BN%7D+(y_i-%5Chat%7By_i%7D)%5E2+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=NLL(x,+y)+=+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi=1%7D%5E%7BN%7D(y_i+-+%5Chat%7By_i%7D)%5E2+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+J_%7BMAE%7D=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Cleft+%7C+y_i+-+%5Chat%7By_i%7D+%5Cright+%7C+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clvert+y-+%5Chat%7By%7D%5Crvert">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmu=0,+b=1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D(-%5Cleft+%7Cy_i-%5Chat%7By_i%7D%5Cright%7C)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x,+y)+=+%5Cprod_%7Bi=1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D(-%7Cy_i-%5Chat%7By_i%7D%7C)%5C%5C+++LL(x,+y)+=+-%5Cfrac%7BN%7D%7B2%7D+-+%5Csum_%7Bi=1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C+%5C%5C+++NLL(x,+y)+=+%5Csum_%7Bi=1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=-%5Chat%7By_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cpm1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clvert+y_i-%5Chat%7By_i%7D+%5Crvert">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-c8edffe0406dafae41a042e412cd3251_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-93ad65845f5b0dc0327fde4ded661804_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=J_%7Bhuber%7D=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5EN%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%5Cleq+%5Cdelta%7D+%5Cfrac%7B(y_i+-+%5Chat%7By_i%7D)%5E2%7D%7B2%7D++%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%3E+%5Cdelta%7D+(%5Cdelta+%7Cy_i+-+%5Chat%7By_i%7D%7C+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdelta">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdelta">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdelta+%5Clvert+y_i+-+%5Chat%7By_i%7D%5Crvert+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Clvert+y+-+%5Chat%7By%7D%5Crvert=%5Cpm+%5Cdelta">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdelta=1.0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B-%5Cdelta,+%5Cdelta%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=(-%5Cinfty,+%5Cdelta)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=(%5Cdelta,+%5Cinfty)">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-b4260d38f70dd920fa46b8717596bda7_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cdelta">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%5Cin+(0,+1)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=NLL(x,+y)=J_%7BCE%7D=-%5Csum_%7Bi=1%7D%5EN%5Cleft+(y_i%5Cmathbb%7Blog(%7D%5Chat%7By_i%7D)+++(1-+y_i)%5Cmathbb%7Blog%7D(1-%5Chat%7By_i%7D)%5Cright)+%5C%5C">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-7e7732b869d7334c2c960c1089b13439_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(y_i=1%7Cx_i)+=+%5Chat%7By_i%7D%5C%5C+++p(y_i=0%7Cx_i)+=+1-%5Chat%7By_i%7D++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+(%5Chat%7By_i%7D)%5E%7By_i%7D+(1-%5Chat%7By_i%7D)%5E%7B1-y_i%7D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(x,+y)=%5Cprod_%7Bi=1%7D%5EN(%5Chat%7By_i%7D)%5E%7By_i%7D+(1-%5Chat%7By_i%7D)%5E%7B1-y_i%7D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=NLL(x,+y)=J_%7BCE%7D=-%5Csum_%7Bi=1%7D%5EN%5Cleft+(y_i%5Cmathbb%7Blog(%7D%5Chat%7By_i%7D)+++(1-+y_i)%5Cmathbb%7Blog%7D(1-%5Chat%7By_i%7D)%5Cright)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=(0,+1)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=(0,+1)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+%5Cprod_%7Bk=1%7D%5EK(%5Chat%7By_i%7D%5Ek)%5E%7By_i%5Ek%7D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k+%5Cin+K">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=NLL(x,+y)+=+J_%7BCE%7D+=+-%5Csum_%7Bi=1%7D%5EN%5Csum_%7Bk=1%7D%5EK+y_i%5Ek+%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=J_%7BCE%7D+=+-%5Csum_%7Bi=1%7D%5EN+y_i%5E%7Bc_i%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5E%7Bc_i%7D)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+KL(p,+q)=%5Csum_%7Bk=1%7D%5EKp%5Ek%5Cmathbb%7Blog%7D(p%5Ek)+-+%5Csum_%7Bk=1%7D%5EKp%5Ek%5Cmathbb%7Blog%7D(q%5Ek)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=KL(y_i%5E%7B%5Cstar%7D,+%5Chat%7By_i%7D)=%5Csum_%7Bk=1%7D%5EKy_i%5E%7B%5Cstar+k%7D%5Cmathbb%7Blog%7D(y_i%5E%7B%5Cstar+k%7D)+-+%5Csum_%7Bk=1%7D%5EKy_i%5E%7B%5Cstar+k%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=-%5Csum_%7Bk=1%7D%5EKy_i%5E%7B%5Cstar+k%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=-+%5Csum_%7Bk=1%7D%5EKy_i%5Ek%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=J_%7BKL%7D+=+-%5Csum_%7Bi=1%7D%5EN%5Csum_%7Bk=1%7D%5EK+y_i%5Ek+%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)=-%5Csum_%7Bi=1%7D%5EN+y_i%5E%7Bc_i%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5E%7Bc_i%7D)+%5C%5C">
<meta property="article:published_time" content="2020-07-19T11:43:45.000Z">
<meta property="article:modified_time" content="2021-09-08T07:30:37.977Z">
<meta property="article:author" content="ZHANGSAISAI">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=J_%7BMSE%7D+=+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D(y_i+-+%5Chat%7By_i%7D)%5E2+%5C%5C">


<link rel="canonical" href="https://zsaisai.github.io/Loss%20Function/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://zsaisai.github.io/Loss%20Function/","path":"Loss Function/","title":"损失函数"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>损失函数 | ZhangSaiSai</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129037882-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-129037882-1","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?d0967e9b160f4f6248804003642ee818"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">ZhangSaiSai</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">二十多岁 人生才刚刚开始</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
        <li class="menu-item menu-item-schedule"><a href="/archives/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E6%A6%82%E5%BF%B5%E5%8C%BA%E5%88%86"><span class="nav-number">1.</span> <span class="nav-text">1. 概念区分</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E5%9B%9E%E5%BD%92%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.</span> <span class="nav-text">2.回归常用损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E5%9D%87%E6%96%B9%E5%B7%AE%E6%8D%9F%E5%A4%B1%EF%BC%88MSE%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 均方差损失（MSE）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E6%8D%9F%E5%A4%B1-MAE"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 平均绝对损失 (MAE)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MAE%E4%B8%8EMSE%E7%9A%84%E6%AF%94%E8%BE%83"><span class="nav-number">2.2.0.1.</span> <span class="nav-text">MAE与MSE的比较</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Huber-Loss"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Huber Loss</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E5%88%86%E7%B1%BB%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">3.分类常用损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 交叉熵损失</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 二分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 多分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="nav-number">3.2.</span> <span class="nav-text">分类为什么是交叉熵</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ZHANGSAISAI"
      src="https://gitee.com/z_saisai/ware_01/raw/master/blog_account/profile.png">
  <p class="site-author-name" itemprop="name">ZHANGSAISAI</p>
  <div class="site-description" itemprop="description">Keep awake,Stay thinking</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zsaisai" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zsaisai" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:z_ss101@163.com" title="E-Mail → mailto:z_ss101@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_41744192?spm=1000.2115.3001.5343" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_41744192?spm&#x3D;1000.2115.3001.5343" rel="noopener" target="_blank"><i class="csdn fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/3564388084" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;3564388084" rel="noopener" target="_blank"><i class="weibo fa-fw"></i>Weibo</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://music.163.com/#/user/home?id=330376567" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;330376567" rel="noopener" target="_blank">Cloudmusic</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.rainymood.com/" title="https:&#x2F;&#x2F;www.rainymood.com&#x2F;" rel="noopener" target="_blank">Rainy Mood</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://paperswithcode.com/task/recommendation-systems" title="https:&#x2F;&#x2F;paperswithcode.com&#x2F;task&#x2F;recommendation-systems" rel="noopener" target="_blank">Papers with Code</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://datawhalechina.github.io/pumpkin-book/#/" title="https:&#x2F;&#x2F;datawhalechina.github.io&#x2F;pumpkin-book&#x2F;#&#x2F;" rel="noopener" target="_blank">Pumpkin-book</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" title="https:&#x2F;&#x2F;tangshusen.me&#x2F;Dive-into-DL-PyTorch&#x2F;#&#x2F;" rel="noopener" target="_blank">DL-Torch</a>
        </li>
    </ul>
  </div>

          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/zsaisai" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zsaisai.github.io/Loss%20Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://gitee.com/z_saisai/ware_01/raw/master/blog_account/profile.png">
      <meta itemprop="name" content="ZHANGSAISAI">
      <meta itemprop="description" content="Keep awake,Stay thinking">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZhangSaiSai">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          损失函数
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-07-19 11:43:45" itemprop="dateCreated datePublished" datetime="2020-07-19T11:43:45+00:00">2020-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="1-概念区分"><a href="#1-概念区分" class="headerlink" title="1. 概念区分"></a>1. 概念区分</h1><p><strong>Loss Function</strong><br>损失函数 Loss Function </p>
<p>损失函数 Loss Function还分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong>。经验风险损失函数指预测结果和实际结果的差别，通常是针对单个训练样本而言，给定一个模型输出  y^ 和一个真实 y ，损失函数输出一个实值损失y - y^ 。用来评价模型的<strong>预测值</strong>和<strong>真实值</strong>不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。结构风险损失函数是指经验风险损失函数加上<strong>正则项</strong>。</p>
<p><strong>Cost Function</strong> </p>
<p>代价函数 Cost Function 通常是针对整个训练集（或者在使用 mini-batch gradient descent 时一个 mini-batch）的总损失 。</p>
<p><strong>Objective Function</strong></p>
<p>目标函数 Objective Function 是一个更通用的术语，表示任意希望被优化的函数，用于机器学习领域和非机器学习领域（比如运筹优化）</p>
<p>即：损失函数和代价函数只是在针对样本集上有区别。</p>
<span id="more"></span>

<h1 id="2-回归常用损失函数"><a href="#2-回归常用损失函数" class="headerlink" title="2.回归常用损失函数"></a>2.回归常用损失函数</h1><h2 id="2-1-均方差损失（MSE）"><a href="#2-1-均方差损失（MSE）" class="headerlink" title="2.1 均方差损失（MSE）"></a>2.1 均方差损失（MSE）</h2><ul>
<li><p>均方差 Mean Squared Error (MSE)损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7BMSE%7D+=+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D(y_i+-+%5Chat%7By_i%7D)%5E2+%5C%5C"></p>
</li>
<li><p>原理推导</p>
<p>实际上在一定的假设下，我们可以使用最大化似然得到均方差损失的形式。假设<strong>模型预测与真实值之间的误差服从标准高斯分布</strong>（正态分布）（ <img src="https://www.zhihu.com/equation?tex=%5Cmu=0,+%5Csigma=1" alt="[公式]"> ），则给定一个 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 模型输出真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 的概率为</p>
</li>
</ul>
<p>  <img src="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+(-%5Cfrac%7B(y_i-%5Chat%7By_i%7D)%5E2%7D%7B2%7D%5Cright+)+%5C%5C" alt="[公式]"></p>
<p>  进一步我们假设数据集中 N 个样本点之间相互独立，则给定所有 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 输出所有真实值 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 的概率，即似然 函数，为所有 <img src="https://www.zhihu.com/equation?tex=p(y_i+%5Cvert+x_i)" alt="[公式]"> 的累乘</p>
<p>  <img src="https://www.zhihu.com/equation?tex=L(x,+y)+=+%5Cprod_%7Bi=1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+(-%5Cfrac%7B(y_i-%5Chat%7By_i%7D)%5E2%7D%7B2%7D%5Cright)+%5C%5C" alt="[公式]"></p>
<p>  通常为了计算方便，我们通常最大化对数似然（通过去log,将乘机转为和的形式,方便计算）</p>
<p>  <img src="https://www.zhihu.com/equation?tex=LL(x,+y)=%5Cmathbb%7Blog%7D(L(x,+y))=-%5Cfrac%7BN%7D%7B2%7D%5Cmathbb%7Blog%7D2%5Cpi+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi=1%7D%5E%7BN%7D+(y_i-%5Chat%7By_i%7D)%5E2+%5C%5C" alt="[公式]"></p>
<p>  去掉与 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 无关的第一项，将最大化对数似然转化为最小化<strong>负</strong>对数似然 ，就可以使用梯度下降等方法对其求最小值了。</p>
<p>  <img src="https://www.zhihu.com/equation?tex=NLL(x,+y)+=+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi=1%7D%5E%7BN%7D(y_i+-+%5Chat%7By_i%7D)%5E2+%5C%5C" alt="[公式]"></p>
<p>  可以看到这个实际上就是均方差损失的形式。也就是说<strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的</strong>，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。</p>
<h2 id="2-2-平均绝对损失-MAE"><a href="#2-2-平均绝对损失-MAE" class="headerlink" title="2.2 平均绝对损失 (MAE)"></a>2.2 平均绝对损失 (MAE)</h2><ul>
<li>平均绝对误差 Mean Absolute Error (MAE)是另一类常用的损失函数，也称为 L1 Loss。其基本形式如下</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=+J_%7BMAE%7D=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Cleft+%7C+y_i+-+%5Chat%7By_i%7D+%5Cright+%7C+%5C%5C" alt="[公式]"></p>
<p>MAE 损失的最小值为 0（当预测等于真实值时），最大值为无穷大。随着预测与真实值绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y-+%5Chat%7By%7D%5Crvert" alt="[公式]"> 的增加，MAE 损失呈线性增长.</p>
<ul>
<li>原理推导</li>
</ul>
<p>同样的我们可以在一定的假设下通过最大化似然得到 MAE 损失的形式，假设<strong>模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution</strong>（ <img src="https://www.zhihu.com/equation?tex=%5Cmu=0,+b=1" alt="[公式]"> ），则给定一个 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 模型输出真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 的概率为</p>
<p><img src="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D(-%5Cleft+%7Cy_i-%5Chat%7By_i%7D%5Cright%7C)+%5C%5C" alt="[公式]"></p>
<p>与上面推导 MSE 时类似，我们可以得到的负对数似然实际上就是 MAE 损失的形式</p>
<p><img src="https://www.zhihu.com/equation?tex=L(x,+y)+=+%5Cprod_%7Bi=1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D(-%7Cy_i-%5Chat%7By_i%7D%7C)%5C%5C+++LL(x,+y)+=+-%5Cfrac%7BN%7D%7B2%7D+-+%5Csum_%7Bi=1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C+%5C%5C+++NLL(x,+y)+=+%5Csum_%7Bi=1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C++%5C%5C" alt="[公式]"></p>
<h4 id="MAE与MSE的比较"><a href="#MAE与MSE的比较" class="headerlink" title="MAE与MSE的比较"></a>MAE与MSE的比较</h4><p><strong>1. MSE 通常比 MAE 可以更快地收敛</strong>。当使用梯度下降算法时，MSE 损失的梯度为 <img src="https://www.zhihu.com/equation?tex=-%5Chat%7By_i%7D" alt="[公式]"> ，而 MAE 损失的梯度为 <img src="https://www.zhihu.com/equation?tex=%5Cpm1" alt="[公式]"> ，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y_i-%5Chat%7By_i%7D+%5Crvert" alt="[公式]"> 很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的。当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了 MSE 在大部分时候比 MAE 收敛地更快。这个也是 MSE 更为流行的原因。</p>
<p><strong>2. MAE 对于 outlier 更加 robust</strong>。我们可以从两个角度来理解这一点：</p>
<p>第一个角度是直观地理解，下图是 MAE 和 MSE 损失画到同一张图里面，由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失。因此当数据中出现一个误差非常大的 outlier 时，MSE 会产生一个非常大的损失，对模型的训练会产生较大的影响。</p>
<p><img src="https://pic2.zhimg.com/80/v2-c8edffe0406dafae41a042e412cd3251_1440w.jpg" alt="img"></p>
<p>​    第二个角度是从两个损失函数的假设出发，MSE 假设了误差服从高斯分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于 outlier 更加 robust，当右图右侧出现了 outliers 时，拉普拉斯分布相比高斯分布受到的影响要小很多。因此以拉普拉斯分布为假设的 MAE 对 outlier 比高斯分布为假设的 MSE 更加 robust。</p>
<p><img src="https://pic1.zhimg.com/80/v2-93ad65845f5b0dc0327fde4ded661804_1440w.jpg" alt="img"></p>
<h2 id="2-3-Huber-Loss"><a href="#2-3-Huber-Loss" class="headerlink" title="2.3 Huber Loss"></a>2.3 Huber Loss</h2><p>上文我们分别介绍了 MSE 和 MAE 损失以及各自的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，Huber Loss则是一种将 MSE 与 MAE 结合起来，取两者优点的损失函数，也被称作 Smooth Mean Absolute Error Loss 。其原理很简单，就是在误差接近 0 时使用 MSE，误差较大时使用 MAE，公式为</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7Bhuber%7D=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5EN%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%5Cleq+%5Cdelta%7D+%5Cfrac%7B(y_i+-+%5Chat%7By_i%7D)%5E2%7D%7B2%7D++%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%3E+%5Cdelta%7D+(%5Cdelta+%7Cy_i+-+%5Chat%7By_i%7D%7C+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2)+%5C%5C" alt="[å¬å¼]"></p>
<p>上式中 <img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="[公式]"> 是 Huber Loss 的一个超参数，<img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="[公式]"> 的值是 MSE 和 MAE 两个损失连接的位置。上式等号右边第一项是 MSE 的部分，第二项是 MAE 部分，在 MAE 的部分公式为 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%5Clvert+y_i+-+%5Chat%7By_i%7D%5Crvert+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2" alt="[公式]">是为了保证误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y+-+%5Chat%7By%7D%5Crvert=%5Cpm+%5Cdelta" alt="[公式]"> 时 MAE 和 MSE 的取值一致，进而保证 Huber Loss 损失连续可导。</p>
<p>下图是 <img src="https://www.zhihu.com/equation?tex=%5Cdelta=1.0" alt="[公式]"> 时的 Huber Loss，可以看到在 <img src="https://www.zhihu.com/equation?tex=%5B-%5Cdelta,+%5Cdelta%5D" alt="[公式]"> 的区间内实际上就是 MSE 损失，在<img src="https://www.zhihu.com/equation?tex=(-%5Cinfty,+%5Cdelta)" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=(%5Cdelta,+%5Cinfty)" alt="[公式]"> 区间内为 MAE损失。</p>
<p><img src="https://pic4.zhimg.com/80/v2-b4260d38f70dd920fa46b8717596bda7_1440w.jpg" alt="img"></p>
<ul>
<li>Huber Loss 的特点</li>
</ul>
<p>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个 <img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="[公式]"> 超参数。</p>
<ul>
<li>Huber Loss python实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># huber 损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber</span>(<span class="params">true, pred, delta</span>):</span></span><br><span class="line">    loss = np.where(np.<span class="built_in">abs</span>(true-pred) &lt; delta , <span class="number">0.5</span>*((true-pred)**<span class="number">2</span>), delta*np.<span class="built_in">abs</span>(true - pred) - <span class="number">0.5</span>*(delta**<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(loss)</span><br></pre></td></tr></table></figure>



<h1 id="3-分类常用损失函数"><a href="#3-分类常用损失函数" class="headerlink" title="3.分类常用损失函数"></a>3.分类常用损失函数</h1><h2 id="3-1-交叉熵损失"><a href="#3-1-交叉熵损失" class="headerlink" title="3.1 交叉熵损失"></a>3.1 交叉熵损失</h2><h3 id="3-1-1-二分类"><a href="#3-1-1-二分类" class="headerlink" title="3.1.1 二分类"></a>3.1.1 二分类</h3><ul>
<li>考虑二分类，在二分类中我们通常使用Sigmoid函数将模型的输出压缩到 (0, 1) 区间内<img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%5Cin+(0,+1)" alt="[公式]"> ，用来代表给定输入 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> ，模型判断为正类的概率。下图是对二分类的交叉熵损失函数的可视化，蓝线是目标值为 0 时输出不同输出的损失，黄线是目标值为 1 时的损失。可以看到约接近目标值损失越小，随着误差变差，损失呈指数增长。</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=NLL(x,+y)=J_%7BCE%7D=-%5Csum_%7Bi=1%7D%5EN%5Cleft+(y_i%5Cmathbb%7Blog(%7D%5Chat%7By_i%7D)+++(1-+y_i)%5Cmathbb%7Blog%7D(1-%5Chat%7By_i%7D)%5Cright)+%5C%5C" alt="[公式]"></p>
<p><img src="https://pic2.zhimg.com/80/v2-7e7732b869d7334c2c960c1089b13439_1440w.jpg" alt="img"></p>
<ul>
<li>原理推导</li>
</ul>
<p>由于只有分两类，因此同时也得到了正负类的概率， 则可以假设样本服从<strong>伯努利分布（0-1分布）</strong> 。</p>
<p><img src="https://www.zhihu.com/equation?tex=p(y_i=1%7Cx_i)+=+%5Chat%7By_i%7D%5C%5C+++p(y_i=0%7Cx_i)+=+1-%5Chat%7By_i%7D++%5C%5C" alt="[公式]"></p>
<p>将两条式子合并成一条</p>
<p><img src="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+(%5Chat%7By_i%7D)%5E%7By_i%7D+(1-%5Chat%7By_i%7D)%5E%7B1-y_i%7D+%5C%5C" alt="[公式]"></p>
<p>假设数据点之间独立同分布，则似然函数，即各样本的概率乘机可以表示为</p>
<p><img src="https://www.zhihu.com/equation?tex=L(x,+y)=%5Cprod_%7Bi=1%7D%5EN(%5Chat%7By_i%7D)%5E%7By_i%7D+(1-%5Chat%7By_i%7D)%5E%7B1-y_i%7D+%5C%5C" alt="[公式]"></p>
<p>对似然取对数，然后加负号变成最小化负对数似然，即为交叉熵损失函数的形式</p>
<p><img src="https://www.zhihu.com/equation?tex=NLL(x,+y)=J_%7BCE%7D=-%5Csum_%7Bi=1%7D%5EN%5Cleft+(y_i%5Cmathbb%7Blog(%7D%5Chat%7By_i%7D)+++(1-+y_i)%5Cmathbb%7Blog%7D(1-%5Chat%7By_i%7D)%5Cright)+%5C%5C" alt="[公式]"></p>
<h3 id="3-1-2-多分类"><a href="#3-1-2-多分类" class="headerlink" title="3.1.2 多分类"></a>3.1.2 多分类</h3><p>在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方有两个：</p>
<ol>
<li><p>真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 现在是一个 One-hot 向量，每个值代表每个类对应的概率，和为1</p>
</li>
<li><p>模型输出的激活函数由原来的 Sigmoid 函数换成 Softmax函数</p>
</li>
</ol>
<p>Softmax 函数将每个维度的输出范围都限定在 <img src="https://www.zhihu.com/equation?tex=(0,+1)" alt="[公式]"> 之间，同时所有维度的输出和为 1，用于表示一个概率分布。</p>
<p>举例：假设一个5分类问题，然后一个样本I的标签y=[0,0,0,1,0]，也就是说样本I的真实标签是4。</p>
<p>假设模型预测的结果概率（softmax的输出）p=[0.1,0.15,0.05,0.6,0.1]，可以看出这个预测是对的，那么对应的损失L=-log(0.6)，也就是当这个样本经过这样的网络参数产生这样的预测p时，它的损失是-log(0.6)。</p>
<p>那么假设p=[0.15,0.2,0.4,0.1,0.15]，这个预测结果就很离谱了，因为真实标签是4，而你觉得这个样本是4的概率只有0.1（远不如其他概率高，如果是在测试阶段，那么模型就会预测该样本属于类别3），对应损失L=-log(0.1)。</p>
<p>再假设p=[0.05,0.15,0.4,0.3,0.1]，这个预测结果虽然也错了，但是没有前面那个那么离谱，对应的损失L=-log(0.3)。我们知道log函数在输入小于1的时候是个负数，而且log函数是递增函数，所以-log(0.6) &lt; -log(0.3) &lt; -log(0.1)。简单讲就是你预测错比预测对的损失要大，预测错得离谱比预测错得轻微的损失要大。 </p>
<ul>
<li>原理推导</li>
</ul>
<p>我们知道Softmax 函数将每个维度的输出范围都限定在 <img src="https://www.zhihu.com/equation?tex=(0,+1)" alt="[公式]"> 之间，同时所有维度的输出和为 1，用于表示一个概率分布。那末对应类的概率可以表示为。</p>
<p><img src="https://www.zhihu.com/equation?tex=p(y_i%7Cx_i)+=+%5Cprod_%7Bk=1%7D%5EK(%5Chat%7By_i%7D%5Ek)%5E%7By_i%5Ek%7D+%5C%5C" alt="[公式]"></p>
<p>假设模型输出<img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]">=[0.1,0.15,0.05,0.6,0.1] ,真实样本为[0,0,0,1,0]，计算得p刚好为0.6,非1项的幂都为1了。</p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=k+%5Cin+K" alt="[公式]"> 表示 K 个类别中的一类，同样的假设数据点之间独立同分布，可得到负对数似然为</p>
<p><img src="https://www.zhihu.com/equation?tex=NLL(x,+y)+=+J_%7BCE%7D+=+-%5Csum_%7Bi=1%7D%5EN%5Csum_%7Bk=1%7D%5EK+y_i%5Ek+%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C" alt="[公式]"></p>
<p>由于 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 是一个 one-hot 向量，除了目标类为 1 之外其他类别上的输出都为 0，因此上式也可以写为</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7BCE%7D+=+-%5Csum_%7Bi=1%7D%5EN+y_i%5E%7Bc_i%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5E%7Bc_i%7D)+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=c_i" alt="[公式]"> 是样本 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 的目标类。通常这个应用于多分类的交叉熵损失函数也被称为 Softmax Loss 或者 Categorical Cross Entropy Loss。</p>
<h2 id="分类为什么是交叉熵"><a href="#分类为什么是交叉熵" class="headerlink" title="分类为什么是交叉熵"></a>分类为什么是交叉熵</h2><p>分类中为什么不用均方差损失？上文在介绍均方差损失的时候讲到实际上均方差损失假设了误差服从高斯分布，在分类任务下这个假设没办法被满足，因此效果会很差。为什么是交叉熵损失呢？有两个角度可以解释这个事情。</p>
<p><strong>一个角度从最大似然的角度，也就是我们上面的推导</strong>，<strong>另一个角度是可以用信息论来解释交叉熵损失</strong></p>
<p>以下是信息论的角度来解释。</p>
<p>假设对于样本 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 存在一个最优分布 <img src="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D" alt="[公式]"> 真实地表明了这个样本属于各个类别的概率，那么我们希望模型的输出 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 尽可能地逼近这个最优分布，在信息论中，我们可以使用 KL 散度来衡量两个分布的相似性。给定分布 <img src="https://www.zhihu.com/equation?tex=p" alt="[公式]"> 和分布 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> ， 两者的 KL 散度公式如下</p>
<p><img src="https://www.zhihu.com/equation?tex=+KL(p,+q)=%5Csum_%7Bk=1%7D%5EKp%5Ek%5Cmathbb%7Blog%7D(p%5Ek)+-+%5Csum_%7Bk=1%7D%5EKp%5Ek%5Cmathbb%7Blog%7D(q%5Ek)+%5C%5C" alt="[公式]"></p>
<p>其中第一项为分布 <img src="https://www.zhihu.com/equation?tex=p" alt="[公式]"> 的信息熵，第二项为分布 <img src="https://www.zhihu.com/equation?tex=p" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 的交叉熵。将最优分布 <img src="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D" alt="[公式]"> 和输出分布<img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 带入 <img src="https://www.zhihu.com/equation?tex=p" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 得到</p>
<p><img src="https://www.zhihu.com/equation?tex=KL(y_i%5E%7B%5Cstar%7D,+%5Chat%7By_i%7D)=%5Csum_%7Bk=1%7D%5EKy_i%5E%7B%5Cstar+k%7D%5Cmathbb%7Blog%7D(y_i%5E%7B%5Cstar+k%7D)+-+%5Csum_%7Bk=1%7D%5EKy_i%5E%7B%5Cstar+k%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C" alt="[公式]"></p>
<p>由于我们希望两个分布尽量相近，因此我们最小化 KL 散度。同时由于上式第一项信息熵仅与最优分布本身相关，因此我们在最小化的过程中可以忽略掉，变成最小化</p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Csum_%7Bk=1%7D%5EKy_i%5E%7B%5Cstar+k%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C" alt="[公式]"></p>
<p>我们并不知道最优分布 <img src="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D" alt="[公式]"> ，但训练数据里面的目标值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 可以看做是 <img src="https://www.zhihu.com/equation?tex=y_i%5E%7B%5Cstar%7D" alt="[公式]"> 的一个近似分布</p>
<p><img src="https://www.zhihu.com/equation?tex=-+%5Csum_%7Bk=1%7D%5EKy_i%5Ek%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)+%5C%5C" alt="[公式]"></p>
<p>这个是针对单个训练样本的损失函数，如果考虑整个数据集，则</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7BKL%7D+=+-%5Csum_%7Bi=1%7D%5EN%5Csum_%7Bk=1%7D%5EK+y_i%5Ek+%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5Ek)=-%5Csum_%7Bi=1%7D%5EN+y_i%5E%7Bc_i%7D%5Cmathbb%7Blog%7D(%5Chat%7By_i%7D%5E%7Bc_i%7D)+%5C%5C" alt="[公式]"></p>
<p>可以看到<strong>通过最小化交叉熵的角度推导出来的结果和使用最大化似然得到的结果是一致的</strong>。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>交叉熵函数与最大似然函数的联系和区别</li>
</ul>
<p>区别：<strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近；<strong>似然函数</strong>的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。</p>
<p>联系：<strong>交叉熵函数</strong>可以由最大似然函数在<strong>伯努利分布</strong>的条件下推导出来，或者说<strong>最小化交叉熵函数</strong>的本质就是<strong>对数似然函数的最大化</strong>。</p>
<ul>
<li><p>分类中的交叉熵损失相当于在数据分布为伯努利分布的情况下的log损失，均方损失相当于数据在高斯分布下的log损失，绝对值损失则是在拉普拉斯分布下的log损失。 </p>
</li>
<li><p>通常在损失函数中还会有正则项（L1/L2 正则），这些正则项作为损失函数的一部分，通过约束参数的绝对值大小以及增加参数稀疏性来降低模型的复杂度，防止模型过拟合。</p>
</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/regularization/" rel="prev" title="常用正则化方法">
                  <i class="fa fa-chevron-left"></i> 常用正则化方法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/probability%20distribution/" rel="next" title="常见概率分布总结">
                  常见概率分布总结 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC80MjE1MS8xODY5OA=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="snowflake-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZHANGSAISAI</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  




  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>

</body>
</html>
