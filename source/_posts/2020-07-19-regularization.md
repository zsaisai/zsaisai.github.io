---
title: 常用正则化方法
date: 2020-07-19 11:43:45
tags: Machine Learning
categories: 机器学习
top:
mathjax: true
---
# 概念

**过拟合与正则化**

在机器学习里，使用少量样本去拟合了所有没见过的样本, 我们叫这种现象为“过拟合”。另外，我们训练模型的数据不可避免的存在一些测量误差或者其他噪音，比如下图中10个点，我们可以找到唯一的9阶多项式 ![[公式]](https://www.zhihu.com/equation?tex=y+%3D+a_0x%5E9+%2B+a_1x%5E8+%2B+...+%2B+a_9) 来拟合所有点；也可以使用线性模型 y = 2x 拟合。

简单来说，正则化是一种为了减小**测试误差**的行为(有时候会增加**训练误差**)。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络，去拟合数据时，很容易出现过拟合现象(训练集表现很好，测试集表现较差)，这会导致模型的泛化能力下降，这时候，我们就需要使用**正则化**，降低模型的复杂度。

![img](https://pic1.zhimg.com/80/v2-74e25eb011e7d911fca43908b50d2644_1440w.jpg)

- 简单总结过拟合的主要几种原因

1. 训练集样本太少
2. 样本中存在噪声
3. 模型过于复杂

从上图可以看出，左侧的拟合结果显然的拟合到所有噪声数据了，所以我们能做的就是使用正则化技术来防止模型学到训练样本中的噪声，从而降低过拟合的可能性，让模型更加 Robust。

一句话：让更复杂模型产生好的效果的同时又不会过拟合，这就是正则化技术的作用。  

<!-- more -->

# 正则化方法
## L2-Norm
L2 规范化是一种最为常用的正则化手段 —— 有时候被称为权重衰减(weight decay)。L2 规范化的想法是增加一个额外的项到代价函数上，这个项叫做规范化项（有时也叫惩罚项）。

![[公式]](https://www.zhihu.com/equation?tex=C+%3DC_0%2B%5Cfrac%7B%5Clambda%7D%7B2n%7D%5Csum_%7Bw%7D%5E%7B%7D%7Bw%5E2%7D)

上式右边第一项是常规的损失函数，第二项就是L2规范化项， ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%3E0) 称为规范化参数，n为训练集合的大小。 直觉地看，规范化的效果是让网络倾向于学习小一点的权重，其他的东西都一样的。大的权重只有能够给出代价函数第一项足够的提升时才被允许。换言之，**规范化可以当做一种寻找小的权重和最小化原始的代价函数之间的折中。这两部分之间相对的重要性就由 λ 的值来控制了: λ 越小，就偏向于最小化原始代价函数，反之，倾向于小的权重**。

在训练神经网络时，我们知道我们的训练目的就是优化权重，而且是使用反向传播和梯度下降来优化的；那么加入正则化项之后权重是如何优化的呢？将上述加L2正则项的代价函数对参数w进行求导：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%7D%3D%5Cfrac%7B%5Cpartial+C_0%7D%7B%5Cpartial+w%7D%2B%5Cfrac%7B%5Clambda%7D%7Bn%7Dw)

于是权重的学习规则（更新规则）变为：

![img](https://pic2.zhimg.com/80/v2-ca26b5fafa9bc4849eb9c3687e7ea9ed_1440w.jpg)

这和正常的梯度下降学习规则相同，只是多了一个因子 ![[公式]](https://www.zhihu.com/equation?tex=1-%5Cfrac%7B%5Ceta+%5Clambda%7D%7Bn%7D) 重新调整了权重w，这种调整称为权重衰减。 粗看，这样会导致权重会不断下降到 0。但是实际不是这样的，因为如果在原始代价函数中造成下降的话其他的项可能会让权重增加。

那么规范化因子 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 如何设置呢？根据调节因子 ![[公式]](https://www.zhihu.com/equation?tex=1-%5Cfrac%7B%5Ceta+%5Clambda%7D%7Bn%7D) ，我们知道，**如果训练数据很多时，也就是n比较大，那么调节因子将接近1，从而使得权重衰减效果不明显，导致规范化效果降低**。**因此，我们可以针对大数据集将规范化参数设置大一点，对小数据集将规范化参数设置的小一些**。

## L1-Norm

L1 规范化是在未规范化的代价函数上加上一个权重绝对值的和:

![img](https://pic4.zhimg.com/80/v2-8454ba668ab26dc19e7a8c9bea0d56ab_1440w.jpg)

凭直觉地看，这和 L2 规范化相似，惩罚大的权重，倾向于让网络优先选择小的权重。当然，
L1 规范化和 L2 规范化并不相同，将上式对参数w进行求偏导：

![img](https://pic1.zhimg.com/80/v2-4c5ac3d63af385afb335f98c615cb3dc_1440w.jpg)

其中 sgn(w) 就是 w 的正负号，即 w 是正数时为 +1，而 w 为负数时为 −1，并约定 sgn(0) = 0。使用这个表达式，我们可以轻易地对反向传播进行修改从而使用基于 L1 规范化的随机梯度下降进行学习。对 L1 规范化的网络进行更新的规则就是

![img](https://pic3.zhimg.com/80/v2-db61571ee32399fbf0798b2309275342_1440w.jpg)

通过和L2规范化的权重更新规则对比，可以看出在 L1 规范化中，权重通过一个常量向 0 进行缩小。在 L2 规范化中，权重通过一个和 w 成比例的量进行缩小的。所以，**当一个特定的权重绝对值 |w| 很大时，L1 规范化的权重缩小得远比 L2 规范化要小得多。相反，当一个特定的权重绝对值 |w| 很小时，L1 规范化的权重缩小得要比 L2 规范化大得多。最终的结果就是：L1 规范化倾向于聚集网络的权重在相对少量的高重要度连接上，而其他权重就会被驱使向 0 接近**。

## Dropout

Dropout 的思想和L1 norm，L2 norm 不同，它并不是通过学习到较小的权重参数来防止过拟合的，它是通过在训练的过程中随机丢掉部分神经元来减小神经网络的规模从而防止过拟合。

这里的丢掉不是永远的丢掉，而是在某一次训练中丢掉一些神经元，这些丢掉的神经元有可能在下一次迭代中再次使用的，因此这里需要和Relu激活函数来做一下区分，Relu激活函数是永久的杀死取值为负的神经元。

那么Dropout具体是如何来防止过拟合的呢？仅仅丢掉一些神经元减少神经网络的规模？也没这么简单啦。下面是Dropout 论文里的例子，也就是发明Dropout技术的动机。

 论文地址：[Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting](http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) 

> 在自然界中，中大型动物中一般是有性繁殖，有性繁殖是指后代的基因从父母两方各继承一半。但是从直观上看，似乎无性繁殖更加合理，因为无性繁殖可以保留大段大段的优秀基因。而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性。但是大自然优胜劣汰，在高等物种中选择没有选择无性繁殖，而选择了有性繁殖，可见有性繁殖的优势。我们先做一个假设，那就是基因的力量在于混合的能力而非单个基因的能力。不管是有性繁殖还是无性繁殖都得遵循这个假设。为了证明有性繁殖的强大，我们先看一个概率学小知识。
> 比如要搞一次恐怖袭击，两种方式：
> \1. 集中50人，让这50个人密切精准分工，搞一次大爆破。
> \2. 将50人分成10组，每组5人，分头行事，去随便什么地方搞点动作，成功一次就算。
> 哪一个成功的概率比较大？ 显然是后者。因为将一个大团队作战变成了游击战。那么，类比过来，有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。

Dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，以达到最好的效果。它减弱了神经元节点间的联合适应性，增强了泛化能力。（ 因为神经元不能依赖其他神经元特定的存在，这个技术其实减少了复杂的互适应的神经元。所以，强制要学习那些在神经元的不同随机子集中更加健壮的特征。）

Dropout通过在每一次训练中，以概率P随机丢掉一些神经元以及之间的链接，这样也就是使得在训练阶段我们训练了很多个不同的小规模神经网络模型，因为每一次训练的神经元个数不同且所留下的神经元也不同。示意图如下：

![img](https://pic3.zhimg.com/80/v2-f8caece61bb5815c0fbac696ea27f2a2_1440w.jpg)

一般P取值为0.5，此时训练的不同的小规模神经网络数目最多，达到 ![[公式]](https://www.zhihu.com/equation?tex=2%5En) 个，n指神经元总个数。上图中左图为标准的神经网络，假设 ![[公式]](https://www.zhihu.com/equation?tex=l%5Cin+%281%2C2%2C..%2CL%29) 表示神经网络的隐藏层， ![[公式]](https://www.zhihu.com/equation?tex=z%28l%29) 表示第l层的输入向量， ![[公式]](https://www.zhihu.com/equation?tex=y%28l%29) 表示l层的输出向量（y(0) = x ，表示输入），W(l) 和 b(l) 指l层的权重和偏置，它的前向传播过程可以描述如下：

![img](https://pic1.zhimg.com/80/v2-f1c01318c8f4600e4678cbff05caa7f4_1440w.jpg)

其中f(.)为激活函数。而对于带 dropout 的右图所示神经网络，它的前向传播过程可以描述如下：

![img](https://pic3.zhimg.com/80/v2-58ea34849c3accd1a9b2ac4816746a7a_1440w.jpg)

图示如下：

![img](https://pic4.zhimg.com/80/v2-ca0b46fd0d51b08cf2acfe48c8eb783f_1440w.jpg)

对任何层l， ![[公式]](https://www.zhihu.com/equation?tex=r%5El) 是一个独立的伯努利向量，向量里元素以概率P取1，以概率1-P取值0，将伯努利向量和每层的输出做点乘，这样就将输出向量中的一部分数值变为0，然后将这个点乘之后的输出向量作为下一层的输入向量，以此往复。

所以实际上Dropout 使得大规模的神经网络在训练时，把总体的那个大的神经网络分成了很多个小规模的神经网络在训练，也就是我们最终得到的模型就是很多个小规模的网络 ensemble 起来的，我们知道 ensemble 的效果是可以提升模型性能也能防止过拟合的。

我们训练好了 ensemble 好多个的小规模的神经网络的模型之后，那么怎么应用于测试呢？直接对指数级数量的小规模细化模型的预测结果取平均吗？答案是不可行的，在实践中，一个近似的取平均方式表现很好。其实在测试时，我们的整个神经网络的每一个神经元都是被保留的，假如一个神经元在训练期间以概率p被保留，那么在在测试时间我们只要将该神经元的输出权重乘以p，如下图所示。

![img](https://pic1.zhimg.com/80/v2-678fbd89d78c1ad41655b2c26fb641f8_1440w.jpg)

概括的说，dropout 其实可以使我们获得一个共享权重的 Bagging 集成模型。我们知道，训练好多模型做 Bagging 是很费时间的，更别说做神经网络的集成了，训练单个神经网络都需要花费很多时间；但是使用了 dropout 技术却可以让我们使用非常少的时间来训练集成模型，虽然这比训练单个模型所花的时间稍微多点，但是相比性能上的提升，这多花的时间是完全值得的。



## Max-Norm

虽然单独使用 dropout 就可以使得模型获得良好表现，不过，如果搭配Max-Norm 的话，那么效果更佳。

对于每一个神经元 Max-Norm Regularization 的目的在于限制输入链接权重的大小，使得 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C_2+%5Cll+r) ，其中 r 是Max-Norm 可调节超参数，||.||_2是L2范数。在每一个 training step 需要计算 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C_2) ，以确保它小于r，如果需要对w进行调整，则使用下式进行调整：

![[公式]](https://www.zhihu.com/equation?tex=w%5Cleftarrow+w%5Cfrac%7Br%7D%7B%7C%7Cw%7C%7C_2%7D)

前面我们讲过，更小的权重参数有利于模型对噪声的鲁棒性，这里限制参数的大小目的就是如此。在实践中将 Max-Norm Regularization 结合dropout 使用一般效果将会更好.

# 思考

- **L1与L2的相同之处。**

  不管是L1还是L2正则，他们的基本思想就是希望通过限制权重的大小，使得模型不能拟合任意的噪声数据，从而达到防止过拟合的目的。 **因为更小的权重意味着网络的行为不会因为我们随便改变了一个输入而改变太大。这会让规范化网络学习局部噪声的影响更加困难**。将它看做是一种让单个的证据不会影响网络输出太多的方式。相对的，规范化网络学习去对整个训练集中经常出现的证据进行反应。对比看，大权重的网络可能会因为输入的微小改变而产生比较大的行为改变。所以一个无规范化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型。简言之，规范化网络受限于根据训练数据中常⻅的模式来构造相对简单的模型，而能够抵抗训练数据中的噪声的特性影响。我们的想法就是这可以让我们的网络对看到的现象进行真实的学习，并能够根据已经学到的知识更好地进行泛化。

  

- **L1与L2的区别。**

   L1和L2正则也有不同之处，L1正则会让参数变得更稀疏，而L2不会。所谓参数变得稀疏是指会有更多的参数变为0，这样可以达到类似特征选取的功能。而之所以L2不会让参数变得稀疏的原因是因为参数很小时，比如0.001，它的平方基本可以忽略，于是模型也就不会进一步的让其变得更小而调整为0。其次，L2正则项可导，L1正则项不可导，这样使得带L2正则项的损失函数更方便更容易优化，而带L1正则项的损失函数的优化就比较复杂。 

   

- **为什么没有对偏置项进行规范化。**

   **值得注意的是规范化中并没有对偏置项进行规范化，因为即使对偏置进行规范化操作也并不会对结果改变太多**，所以，在某种程度上，对不对偏置进行规范化其实就是一种习惯了。然而，需要注意的是，**有一个大的偏置并不会像大的权重那样会让神经元对输入太过敏感**。所以我们不需要对大的偏置所带来的学习训练数据的噪声太过担心。同时，允许大的偏置能够让网络更加灵活 —— 因为，**大的偏置让神经元更加容易饱和，这有时候是我们所要达到的效果**。所以，我们通常不会对偏置进行规范化。

   

- **直接使用简单的模型能达到防止过拟合效果吗 ？**

   以上看，貌似模型越复杂就越可能导致过拟合，那么我们直接使用简单的模型行不行呢？行当然是行的，但是小规模的神经网络模型效果就不那么好了。比如，如果是MNIST手写数字识别数据集，我们只要使用两层隐藏层的神经网络，并且每层网络的神经元只要100个或者300个就可以得到良好的效果。但是如果是ImageNet那样的数据集呢？显然浅层神经网络就不行了，为此有了152层的ResNet。所以面对庞大的数据集我们的目标还是要足够大的。

   **因此针对不同数据集，我们设计网络结构时，应该设计出比你实际需要的更多层数、更多神经元的网络结构，然后使用规范化技术去防止过拟合**。这样可以节省做实验去寻找最佳层数最佳神经元个数的时间，也能更快的得到更好的结果。

   

- **为什么深层神经网络比浅层神经网络更好 ？**

1. DNN具有更多神经元，因此具有更多参数，这使得它可以拟合更加复杂的各种函数，具有更好好的表达能力
2. 现实世界的数据大多都是以分层的结构构造的，比如人脸识别任务，较低层次是各种形状和方向的线条（轮廓），中间层次就是这些线条组成的方形或圆形（五官），高层次的特征就是五官组成的人脸了，DNN的不同层从浅层到高层可以依次提取不同层级的特征
3. DNN在新数据集上更具有健壮的泛化能力。

即：**我们要利用深层神经网络的优势又要避免它所带来的过拟合问题**。



- **其他**

  *岭回归(Ridge Regression)：即加了L2-norm正则化的线性回归*

  *LASSO回归：即加了L1-norm的惩罚项的线性回归。*